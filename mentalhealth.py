# -*- coding: utf-8 -*-
"""MentalHealth_type2 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CaljbPkQ2-ZlV1_HpeVanPdi1-Qg67Cz
"""

#download data from kaggle
!pip install opendatasets

import pandas as pd
import numpy as np
import opendatasets as od
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn

# imputing missing values in num_cols
from sklearn.impute import SimpleImputer
# scaling numeric value in 1-0 range
from sklearn.preprocessing import MinMaxScaler,FunctionTransformer

# encode categorical cols in one hot encoder
from sklearn.preprocessing import OneHotEncoder

# Combine the numerical and categorical pipelines
from sklearn.compose import ColumnTransformer

od.download("https://www.kaggle.com/competitions/playground-series-s4e11/data")
# hemantk777   721b095dacb7cfa6129768282921b8fb

MH_data=pd.read_csv("/content/playground-series-s4e11/train.csv")

test_data=pd.read_csv("/content/playground-series-s4e11/test.csv")
#test_data.head(2)
MH_data.head()

MH_data.info()

MH_data.isnull().sum()

#removing same profession as name like yogesh name and yogesh as profession & places
MH_data=MH_data[MH_data['Profession'] != MH_data['Name']]
MH_data=MH_data[~MH_data['Profession'].isin(["Pranav","Yogesh","Nagpur","Patna","Visakhapatnam"])]

#for test data
#test_data=test_data[test_data['Profession'] != test_data['Name']]
#test_data=test_data[~test_data['Profession'].isin(['24th', 'Manvi', 'Samar', 'Surat','No','Name'])]


# profession analysis
a=MH_data.groupby('Profession')['id'].count().reset_index().sort_values(by="id",ascending=False)
b=pd.DataFrame(a)

#top most profession
sns.barplot(y='id',x='Profession',data=b.head(30))
plt.xticks(rotation=45)     # if x-axis has text labels
plt.tight_layout()
plt.rcParams['figure.figsize'] = (22,6)
plt.show()

#test_data['Profession'].unique()

#proffesion wise depression
sns.barplot(x=MH_data['Profession'],y=MH_data["id"],hue=MH_data['Depression'],data=MH_data)
plt.xticks(rotation=45)     # if x-axis has text labels
plt.tight_layout()
plt.rcParams['figure.figsize'] = (25,12)
plt.show()

sns.countplot(x=MH_data['Sleep Duration'],data=MH_data)
plt.xticks(rotation=45)     # if x-axis has text labels
plt.tight_layout()
plt.rcParams['figure.figsize'] = (25,12)
plt.show()

#removing outlier by observing above charts
MH_data=MH_data[MH_data['Sleep Duration'].isin(["More than 8 hours","Less than 5 hours","5-6 hours","7-8 hours","3-4 hours"])]

#for test data
#test_data=test_data[test_data['Sleep Duration'].isin(["More than 8 hours","Less than 5 hours","5-6 hours","7-8 hours","3-4 hours"])]

MH_data['Sleep Duration'].unique()
test_data['Sleep Duration'].unique()

sns.countplot(x=MH_data['City'],data=MH_data)
plt.xticks(rotation=45)     # if x-axis has text labels
plt.tight_layout()
plt.rcParams['figure.figsize'] = (25,12)
plt.show()

z=MH_data.groupby("City")["id"].count().reset_index().sort_values(by='id',ascending=False)
z.head(30)         # after 30 there are unnecessary entry
MH_data=MH_data[MH_data['City'].isin(z['City'].head(30))]
MH_data['City'].unique()

#test_data=test_data[test_data['City'].isin(z['City'].head(30))]
#test_data['City'].unique()

MH_data['Working Professional or Student'].unique()



#removing name col because it is not used is in our model
MH_data=MH_data.drop("Name",axis=1)

test_data=test_data.drop("Name",axis=1)
test_data.head(1)

#selecting input and target columns name
input_cols=MH_data.columns[1:-1].to_list()
target_col ='Depression'

#assign input and target columns
train_input=MH_data[input_cols].copy()   # train
train_target=MH_data[target_col].copy()


test_input=test_data[input_cols].copy()  #test
train_input.shape

#test_data[input_cols].count()

test_input.columns

# numerical cols and categorical cols
cat_cols=train_input.select_dtypes(include='object').columns.tolist()
num_cols=train_input.columns[~train_input.columns.isin(cat_cols)].tolist()   # logic for numeric col after getting cat_cols
#cat_cols
#num_cols

# create pipeline for num and cat cols
from sklearn.pipeline import Pipeline
# num cols
num_pipeline=Pipeline([
    ('imputer',SimpleImputer(strategy='mean')),
    ('scaler',MinMaxScaler()),
    ('convert_to_float32', FunctionTransformer(lambda x: x.astype(np.float32)))
])

cat_pipeline=Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('encoder',OneHotEncoder(sparse_output=False,handle_unknown='ignore'))
])

preposessor=ColumnTransformer(
    transformers=[
        ('num',num_pipeline,num_cols),
        ('cat',cat_pipeline,cat_cols)
    ]
)

# load datasets (processed)
x_train=preposessor.fit_transform(train_input)
x_test = preposessor.transform(test_input)

x_train



#   gradient boosting (  xgb classifier model  )
from xgboost import XGBClassifier
model1=XGBClassifier(random_state=11,n_jobs=-1,objective='binary:logistic',eval_metric='logloss',colsample_bytree=0.8,subsample=0.9,gamma=0.1,
                     n_estimetors=400,max_depth=3,learning_rate=0.3,)
print('train model1')

#   catboost (  catboost classifier model  )
!pip install catboost

from catboost import CatBoostClassifier
model2 = CatBoostClassifier(iterations=200, learning_rate=0.3, depth=7, random_seed=11, subsample=0.9,
                            colsample_bylevel=0.8, l2_leaf_reg=0.1, eval_metric='Logloss', loss_function='Logloss', verbose=0, task_type='CPU')
print('train model2')

# Define stacking ensemble with the LightGBM model tuned in this trial
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

stacking_ensemble = StackingClassifier(
    estimators=[
        ('catboost', model2),
        ('xgb', model1)
    ],
    final_estimator=LogisticRegression(),
    passthrough=False,
    n_jobs=-1
)

stacking_ensemble

# evaluation model function
from sklearn.metrics import accuracy_score,make_scorer

# Define a scoring metric
scoring = make_scorer(accuracy_score)

# Perform cross-validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(model1, x_train, train_target, cv=5, scoring=scoring)

print("CV Accuracy scores :",cv_scores)
print("Mean CV Accuracy:",cv_scores.mean())

#fit the model
model1.fit(x_train, train_target)

test_preds=model1.predict(x_test)

#for submission
sub_df=pd.read_csv('/content/playground-series-s4e11/sample_submission.csv')
sub_df['Depression']=test_preds
sub_df.to_csv('xgb_structured.csv',index=None)

def pred_input(model,single_input):
  input_df=pd.DataFrame(single_input)
  input_df=input_df.drop("Name",axis=1)
  input_cols=input_df.columns[1:].to_list()
  input_df=input_df[input_cols]

  x_single_input = preposessor.transform(input_df)
  pred=model1.predict(x_single_input)
  prob=model1.predict_proba(x_single_input)

  lable='Depressed' if int(pred[0]) else 'Not Depressed'
  probability=prob[0][pred[0]]
  return f'{lable} with {probability:.4f} % sure'

from math import nan

single_inpute={
    'id': np.nan,
    'Name':'Rohan',
    'Gender': 'Female',
    'Age': 26.0,
    'City': 'Varanasi',
    'Working Professional or Student': 'Working Professional',
    'Profession':'Teacher',
    'Academic Pressure':np.nan,
    'Work Pressure':5.0,
    'CGPA':np.nan,
    'Study Satisfaction':np.nan,
    'Job Satisfaction':2.0,
    'Sleep Duration':'Less than 5 hours',
    'Dietary Habits':'Moderate',
    'Degree':'LLB',
    'Have you ever had suicidal thoughts ?':'No',
    'Work/Study Hours':8.0,
    'Financial Stress':2.0,
    'Family History of Mental Illness':'Yes'
}

1	Vivan	Male	26.0	Varanasi	Working Professional	Teacher	NaN	4.0	NaN	NaN	3.0	Less than 5 hours	Unhealthy	LLB	Yes	7.0	3.0	No	1

pred_input(model1,[single_inpute])